{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"THANK YOU FOR REVIEWING THIS NOTEBOOK\n",
    "   Please email me so I know your a real person and can help with further code.\n",
    "   Remainder of this file builds a Panda DF, analyzes basic sentiment with Vadar finishing with Naive Bayes Prediction\n",
    "   Much apprecated ~BBE\n",
    "   brian.p.hogan@alumni.harvard.edu\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Created on Wed Jul 10 15:06:41 2019\n",
    "@author: BBE - Brian Hogan\n",
    "Objective: Generate New York State twitter traffic chatter building profile\n",
    "           of good, bad, and ugly traffic pattern days.\n",
    "Method: \n",
    "    Obtain: Mongodb grab tweets over month across 1 to n twitter handles. \n",
    "    Scrub:  Pandas dataframe.\n",
    "    Analyze: NLTK w Vadar for +/- neu and compound scoring\n",
    "    Visualize: Wordcloud\n",
    "    Predict: Naive Bayes Sentiment Analysis\n",
    "\"\"\"\n",
    "import tweepy  \n",
    "import json\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from bson.json_util import dumps  #from dn_fn.py for save & load to database\n",
    "\n",
    "CONSUMER_KEY = 'GFuEK46t.....'  #BBE twitter keys...\n",
    "CONSUMER_SECRET = 'sWsBF6S9EOPD.....'\n",
    "OAUTH_TOKEN = '989685004832792578-3....'\n",
    "OAUTH_SECRET = 'zRm1pwVBQOYX4b8...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Functions\"\"\"\n",
    "\"\"\"=> twitter login        \"\"\"\n",
    "def oauth_login():\n",
    "  auth = tweepy.OAuthHandler(CONSUMER_KEY,CONSUMER_SECRET)\n",
    "  auth.set_access_token(OAUTH_TOKEN,OAUTH_SECRET)\n",
    "  tweepy_api = tweepy.API(auth)\n",
    "  if (not tweepy_api):        #error out\n",
    "      print (\"Problem Connecting to API with OAuth\")\n",
    "  return tweepy_api  #api object to twitter functions\n",
    "def appauth_login(): #login to twitter w extended rate limiting\n",
    "  auth = tweepy.AppAuthHandler(CONSUMER_KEY,CONSUMER_SECRET)\n",
    "  #auth.set_access_token(OAUTH_TOKEN,OAUTH_SECRET) #needed for one test so put back in\n",
    "  tweepy_api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "  if (not tweepy_api):  #let user know if api error\n",
    "      print (\"Problem Connecting to API with AppAuth\")\n",
    "  return tweepy_api    #api object to twitter functions\n",
    "\n",
    "\"\"\"=> connection test \"\"\"\n",
    "if __name__ == '__main__':  #test connection\n",
    "  tweepy_api = oauth_login()\n",
    "  print (\"Twitter  Authorization OK :\", tweepy_api)\n",
    "  tweepy_api = appauth_login()\n",
    "  print (\"Twitter  Authorization OK :\", tweepy_api) \n",
    "\n",
    "def simple_search(api, query, max_results=20):  #ASYNCH 8.4\n",
    "    # the first search initializes a cursor, stored in the metadata results,\n",
    "  #   that allows next searches to return additional tweets\n",
    "  search_results = [status for status in tweepy.Cursor(api.search, q=query).items(max_results)]  \n",
    "  tweets = [tweet._json for tweet in search_results]\n",
    "  return tweets\n",
    "\n",
    "\"\"\"asynch dn_fn.py  \"\"\"\n",
    "def save_to_DB(DBname, DBcollection, data):    \n",
    "    client = pymongo.MongoClient('localhost', 27017) #connect to server\n",
    "    \"\"\"change names to lowers case because they are not case senstitive\n",
    "    and remove special characteers like hashtask and spaces   \"\"\"  \n",
    "    DBname = DBname.lower()\n",
    "    DBname = DBname.replace('#', '')\n",
    "    DBname = DBname.replace(' ', '')\n",
    "    DBcollection = DBcollection.lower()\n",
    "    DBcollection = DBcollection.replace('#', '')\n",
    "    DBcollection = DBcollection.replace(' ', '')\n",
    "    db = client[DBname]\n",
    "    collection = db[DBcollection]   \n",
    "    collection.insert_many(data)\n",
    "    print(\"\\nSaved\", len(data), \"documents to DB\", DBname, DBcollection)\n",
    "\n",
    "\"\"\"dn_fn.py  - used to get existing data; return as json objects\"\"\"\n",
    "def load_from_DB(DBname, DBcollection):\n",
    "    client = pymongo.MongoClient('localhost', 27017) \n",
    "    client.list_database_names   # ISSUE HERE W DEPRECTATION again...5-31-19\n",
    "    db = client[DBname]\n",
    "    collection = db[DBcollection]  #find collection and load docs  \n",
    "    docs = collection.find()\n",
    "    docs_bson = list(docs)\n",
    "    docs_json_str = [dumps(doc) for doc in docs_bson]\n",
    "    docs_json = [json.loads(doc) for doc in docs_json_str]\n",
    "    return docs_json  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get Tweets from MongoDB and store in Panda Frame\"\"\"\n",
    "\"\"\"8.4 \"\"\"\n",
    "if __name__ == '__main__':\n",
    "    print(\"Program collects twitter tweets generating wordclouds,freqency, and sentiment; requires a MongoDB.\")\n",
    "    \"\"\"ask user for hashag, database and dbcollection so not hardcoded\"\"\"\n",
    "    print(\"............................................................................................\")\n",
    "    print(\"Please select one of the Following Artificial Intelligence Twitter experts for this program.\")\n",
    "    print(\"............................................................................................\")\n",
    "    print(\"====>@mfordfuture<===========\") #,@romanyam,@cynthiabreazel,@petitegeek,@erickorvitz,@FLIxrisk,@FHIOxford\")\n",
    "    print(\"............................................................................................\")\n",
    "    query = input(\"Enter Twitter hashtag (#, @ etc): \")\n",
    "    print(\"...........................................................................................\")\n",
    "    num_tweets = input(\"Enter max # of tweets to grab: \")\n",
    "    num_tweets = int(num_tweets)\n",
    "    print(\"...........................................................................................\")\n",
    "    DBname = input(\"Enter mongodb name (this query doesnt overwrite old data): \")\n",
    "    print(\"...........................................................................................\")\n",
    "    DBcollection = input(\"Please Mongo filename to store within your database: \")\n",
    "    \n",
    "    api = appauth_login()  #login to thr api\n",
    "    #api = oauth_login() <--uncomment if swtich to appauth to avoid rate limit\n",
    "\n",
    "    result_tweets = simple_search(api, query, max_results=num_tweets)\n",
    "    print ('Number of result tweets imported: ', len(result_tweets)) #let user know success\n",
    "   \n",
    "    save_to_DB(DBname, DBcollection, result_tweets)  #save to database\n",
    "    \n",
    "    \"\"\"OK now that we have the tweets were going to do some counting\"\"\"\n",
    "    print('Tweet summary statistics are next. Refer to the tweet-datatable.txt '\\\n",
    "          'output file in the folder run for full tweet dataset collected.') \n",
    "    #get results from mongo db\n",
    "    tweet_results = load_from_DB(DBname.lower(), DBcollection.lower())\n",
    "    tweet_df = pd.DataFrame() #initiate an empty dataframe to fill\n",
    "    tweet_df['id']=[tweet['id'] for tweet in tweet_results] #collect data\n",
    "    tweet_df['language']=[tweet['lang'] for tweet in tweet_results]\n",
    "    tweet_df['location']=[tweet['user']['location'] for tweet in tweet_results]\n",
    "    tweet_df['screen_name']=[tweet['user']['screen_name'] for tweet in tweet_results]\n",
    "    tweet_df['followers']=[tweet['user']['followers_count']for tweet in tweet_results]\n",
    "    tweet_df['tweet']=[tweet['text']for tweet in tweet_results]\n",
    "    df2 = pd.DataFrame(tweet_df)\n",
    "    #what data is provided to customer\n",
    "    print(\"Tweet columns in the csv output reports include: \",df2.columns)\n",
    "    \n",
    "    #import summary statistics\n",
    "    #print(\"What are unique total counts, unique values, top values of tweets? :{}\".format(df2.describe(include=['object'])))\n",
    "    #this meta data could be parsed - need to learn how to execute\n",
    "    print(\"............................................................................................\")\n",
    "    print(\"Tweet import metadata :{}\".format(df2.sum()))  #metadata of all tweets\n",
    "    print(\"............................................................................................\")\n",
    "    output_tweet_data = df2.describe(include=['object']) #output detail to csv\n",
    "    output_tweet_data.to_csv(\"Final_project_Tweets_Dataframe_BBE.txt\", index=True)\n",
    "    #average followers\n",
    "    #print(\"What are the average total tweet followers :{}\".format(df2.describe()))\n",
    "    output_tweet_data = df2  #output the total tweet datatable\n",
    "    output_tweet_data.to_csv(\"Final_project_Tweets_BBE.txt\", index=True)\n",
    "    \n",
    "    \"\"\"===WORD FREQUENCY==================\"\"\"\n",
    "    import nltk  #for natural language modeling\n",
    "    nltk.download('stopwords')\n",
    "    client = pymongo.MongoClient('localhost', 27017) \n",
    "    client.list_database_names()   # ISSUE HERE W DEPRECTATION again...5-31-19\n",
    "    #client.list_database_names()  \n",
    "    #project is 652(cant use - made bk, bkf the file)\n",
    "    \"\"\"=============\"\"\"\n",
    "    db = client.DBname #client.bk\n",
    "    db.collection_names()  #get the collection name\n",
    "    collection = DBcollection       #db.bk_f  #find collection and load docs \n",
    "    \"\"\"================\"\"\"\n",
    "    \"\"\" THE FOLLOWING is what you use to go get the tweets and carry on\"\"\"\n",
    "    docs = load_from_DB(DBname, DBcollection)\n",
    "    doclist = [tweet for tweet in docs]\n",
    "    #len(doclist)\n",
    "    def print_tweet_data(tweets):    #sample loop to read through tweets\n",
    "        for tweet in tweets:\n",
    "            print('\\nDate: ',tweet['text'])\n",
    "            #print_tweet_data(doclist[:1])\n",
    "    \"\"\"important to build the message list\"\"\"\n",
    "    msglist = [doc['text'] for doc in doclist if 'text' in doc.keys()]\n",
    "    #len(msglist)\n",
    "    \"\"\"tokens are a summary of individual words\"\"\"\n",
    "    all_tokens = [tok for msg in msglist for tok in nltk.word_tokenize(msg)]\n",
    "    #len(all_tokens)\n",
    "    #all_tokens[:10]\n",
    "    msgtweet = nltk.FreqDist(all_tokens) #build the frequency of tokenized words\n",
    "    #msgtweet.most_common(15)\n",
    "    all_tokens = [tok.lower() for msg in msglist for tok in nltk.word_tokenize(msg)]\n",
    "    #all_tokens[:10]\n",
    "    nltk_stopwords = nltk.corpus.stopwords.words('english')#remove nonvalue add words\n",
    "    #len(nltk_stopwords)\n",
    "    import re\n",
    "    def alpha_filter(w):\n",
    "        pattern = re.compile('^[^a-z]+S')  #need to expand on filter for more\n",
    "        if (pattern.match(w)):             #symbols\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    token_list = [tok for tok in all_tokens if not alpha_filter(tok)]\n",
    "    #token_list[:30]\n",
    "    msgtweet = nltk.FreqDist(token_list)\n",
    "    top_words=msgtweet.most_common(20) #words used most in the tweets\n",
    "    #words={} #make a dictionary  ====>move to dictionary in future\n",
    "#    print(\"Twitter Traffic Chatter Most Common Words/Frequency\")\n",
    "#    for word, freq in top_words:   #print the most commone words\n",
    "#        print(\"Word:\",word,freq)\n",
    "    # close the database connection\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"=> TWEET PUll =>STRIP & CLEAN =>STOPWORDS =>SENTIMENT => TOKENIZE GRID?\n",
    "    1) Tweet Pull from Mongo DB:    ( 652 )\n",
    "        (tweet pull manual from top 10 AI twitter hashtags from newsarticle)\n",
    "    2) Strip & Clean:               \n",
    "    3) Stopwords Remove:            ibid\n",
    "    4) Sentiment:                   ibid\n",
    "    5) Tokenize Pos/Neg grid        ibid   \n",
    "    6) POS/NEG Word Clouds                            \"\"\"\n",
    "        \n",
    "\"\"\"==========================================================================\n",
    "==> 1 ) Tweet Pull (from Mongo Database)\n",
    "===========================================================================\"\"\"\n",
    "#Getting the msglist from Mongo database - raw with uncleaned garbage!\n",
    "sentences = msglist  #msglist = [doc['text'] for doc in doclist if 'text' in doc.keys()]\n",
    "mytweets = []\n",
    "for sentence in sentences:  #from Dr. GAtes\n",
    "    mytweets.append(sentence)\n",
    "#print(mytweets)\n",
    "\"\"\"==========================================================================\n",
    "==> 2 ) Strip & Clean  (w emojies)\n",
    "==========================================================================\"\"\"\n",
    "import re\n",
    "#ONline search to remove emojois as don't know how to handle all that yet\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                             u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                             u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                             u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                             u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                             \"]+\", flags=re.UNICODE)\n",
    "emoji_pattern2 = re.compile(\n",
    "        u\"(\\ud83d[\\ude00-\\ude4f])|\"  # emoticons\n",
    "        u\"(\\ud83c[\\udf00-\\uffff])|\"  # symbols & pictographs (1 of 2)\n",
    "        u\"(\\ud83d[\\u0000-\\uddff])|\"  # symbols & pictographs (2 of 2)\n",
    "        u\"(\\ud83d[\\ude80-\\udeff])|\"  # transport & map symbols\n",
    "        u\"(\\ud83c[\\udde0-\\uddff])\"  # flags (iOS)\n",
    "        \"+\", flags=re.UNICODE)\n",
    "single_parenthsis = re.sub(\"'\",\"\",\"A single ' char\")\n",
    "myfinaltweets=[]\n",
    "for line in mytweets:\n",
    "    #print(\"The next line is, \",line)\n",
    "    line = line.rstrip()  #strip whitespace from the end\n",
    "    line = re.sub('[/:?;!@#$-.]','',line) #adding colons and question mark\n",
    "    line = re.sub('[...]','',line) #adding ellipsis\n",
    "    # line = re.sub('[\\\\']','',line) #adding ellipsis\n",
    "    line = re.sub(\"'\",\"\",line)\n",
    "    \n",
    "    line = re.sub('\\s+',' ',line).strip() #remove extra whitespace\n",
    "    line = line.strip(\"\\n\") #remove new line\n",
    "    line = line.lower()\n",
    "    line = emoji_pattern.sub(r'', line) #emoji_pattern.sub(r'', text)) # no emoji\n",
    "    line = emoji_pattern2.sub(r'', line)  #stil have some trouble icons\n",
    "    #now remove for other characters not being pulled out!\n",
    "    line = re.sub('https','',line)\n",
    "    line = re.sub('mfordfuture','',line)\n",
    "    line = re.sub('kdnuggets','',line)\n",
    "    #print(\"Now the line is: \",line)\n",
    "    myfinaltweets.append(line)\n",
    "print(\"............................................................................................\")  \n",
    "print(\".......Total tweets in analysis :\",len(myfinaltweets) ) \n",
    "print(\"............................................................................................\")\n",
    "#myfinaltweets[:1]\n",
    "#myfinaltweets\n",
    "#print(myfinaltweets)\n",
    "finaltweetsjoined = \"\".join(myfinaltweets) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"==================================================================\n",
    "==> 3) StopWords & Wordcloud\n",
    "===================================================================\"\"\"\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#text=\"This is any sentence of text. It can have punctuation, CAPS!, etc.\"\n",
    "tokenized_word=word_tokenize(finaltweetsjoined)\n",
    "#len(tokenized_word)\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "filtered_tweet_words=[]\n",
    "for w in tokenized_word:\n",
    "    #print(w)\n",
    "    if w not in stop_words:\n",
    "        filtered_tweet_words.append(w)\n",
    "        \n",
    "#print(\"Tokenized text:\",tokenized_word)\n",
    "#print(filtered_tweet_words)\n",
    "print(\"............................................................................................\")\n",
    "print(\"# of Unfiltered Word Tokens in Tweets Pulled:\",len(tokenized_word))\n",
    "print(\"............................................................................................\")\n",
    "print(\"# of Filtered Word Token, w No StopWords, in Tweets Pulled:\",len(filtered_tweet_words))    \n",
    "print(\"............................................................................................\")\n",
    "\"\"\"still have bad characters that cant export ! \"\"\"\n",
    "    #tweetfile= open('HW1_tweets.txt','r')\n",
    "    #with open('HW1_tweets.txt',\"w\") as f:\n",
    "    #    for item in filtered_text:\n",
    "    #        f.write(\"%s\\n\" %item)\n",
    "    #tweetfile.close()\n",
    "\"\"\"==================================================================\n",
    "==> 3) word Frequency\n",
    "===================================================================\"\"\"\n",
    "mostfrequentwords = nltk.FreqDist(filtered_tweet_words)\n",
    "top_words=mostfrequentwords.most_common(50) #words used most in the tweets\n",
    "#top_words\n",
    "#len(top_words)\n",
    "#well can make one but still not helping with getting homework done!\n",
    "import pandas as pd\n",
    "DF_topwords = pd.DataFrame(top_words)\n",
    "print(\"............................................................................................\")\n",
    "print(\"................50 Top Words from Tweets............... \\n\",DF_topwords)       #print(\"HW1-736- AI Most Frequent Words... \\n\")\n",
    "print(\"............................................................................................\")\n",
    "output_tweet_data = DF_topwords  #output the total tweet datatable\n",
    "print(\"Top Words Stored to Documents as : BBE_HW1_ist736_Tweet_Word_Frequency.txt\")\n",
    "output_tweet_data.to_csv(\"BBE_HW1_736_Tweet_Frequency.txt\", index=True)\n",
    "\n",
    "\"\"\"==================================================================\n",
    "==> 3) WordCloud (join tweets back together to create a wordcloud from grp)\n",
    "===================================================================\"\"\"\n",
    "wordcloud_items=[] #make a dictionary  ====>move to dictionary in future\n",
    "for word, freq in top_words:   #print the most commone words\n",
    "        #print(\"Word:\",word,freq)\n",
    "        wordcloud_items.append(word)\n",
    "#print(wordcloud_items)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "#>conda install -c conda-forge wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "joinedfilteredtweets = \" \".join(filtered_tweet_words)  ##  join\n",
    "#print(joinedfilteredtweets)  # lower max_font_size, change the maximum number of word and \n",
    "    #lighten the background:\"\"\"                                 #white, purple, etc \n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"lightblue\").generate(joinedfilteredtweets)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"===================================================================\n",
    "    SENTIMMENT ANALYUSIS => VADAR\n",
    "=======================================================================\"\"\"\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#this analyzer expects a list of text sentences\n",
    "# provides positive, negative, and neutral. It also gives a compound\n",
    "#score, which should be the overall sentiment, ranging -1 to +1 (positive).\n",
    "\n",
    "sentences = myfinaltweets\n",
    "#myfinaltweets\n",
    "#len(myfinaltweets)\n",
    "sid=SentimentIntensityAnalyzer()\n",
    "score = []\n",
    "for sentence in sentences:\n",
    "    #print(sentence)\n",
    "    ss=sid.polarity_scores(sentence)\n",
    "    for k in sorted(ss):\n",
    "        #print('{0}:{1},'.format(k,ss[k]),end=' ') #single quote not a double\n",
    "        score.append(ss)\n",
    "#creteing a data frame with all the sentiment scores so I can aggregate them\n",
    "import pandas as pd\n",
    "score_result = pd.DataFrame(score)\n",
    "score_result  #uncomment it want to see the dataframe\n",
    "    #      compound    neg    neu    pos\n",
    "    #0       0.8104  0.076  0.557  0.368\n",
    "\"\"\"score the whole dataframe for a net pos/neg result\"\"\" \n",
    "#https://cmdlinetips.com/2018/12/how-to-loop-through-pandas-rows-or \\\n",
    " #                                   -how-to-iterate-over-pandas-rows/\n",
    "pos_value=0\n",
    "neg_value=0\n",
    "no_count=0\n",
    "pos_count=0\n",
    "neg_count=0\n",
    "for index, row in score_result.iterrows():\n",
    "    pos_value=0\n",
    "    neg_value=0\n",
    "    pos_value = (index,row['pos'])         \n",
    "    neg_value = (index,row['neg'])\n",
    "    if pos_value != neg_value:\n",
    "        if (pos_value > neg_value):\n",
    "            pos_count = pos_count+1\n",
    "        if (pos_value <= neg_value):\n",
    "            neg_count = neg_count+1\n",
    "    if pos_value == neg_value:  \n",
    "            no_count=no_count+1\n",
    "no_count\n",
    "pos_count\n",
    "neg_count\n",
    "print(\"..........Vadar Sentiment Score: Positive, Negative, Neutral..........\")\n",
    "print(pos_count/len(score_result)),print(neg_count/len(score_result)),print(no_count/len(score_result))\n",
    "if((pos_count/len(score_result))>neg_count/len(score_result)):\n",
    "    print(\"......................OVERALL VADAR SENTIMENT POSITIVE ! \")\n",
    "if((pos_count/len(score_result))<=neg_count/len(score_result)):\n",
    "    print(\"......................OVERALL VADAR SENTIMENT NEGATIVE ! \")\n",
    "\"\"\"===================================================================\n",
    "    SENTIMMENT ANALYUSIS => naiveBayes\n",
    " =================================================================== \"\"\"\n",
    "import operator\n",
    "split_docs_in_half = int(round(len(myfinaltweets)/2,0))\n",
    "split_docs_in_half\n",
    "a=operator.__index__(split_docs_in_half)\n",
    "#a\n",
    "bbe_subj_docs=[] #need a list to put the tuples in to run nB sentiment\n",
    "mydict = {} \n",
    "for line in myfinaltweets[0:a]:  #this will print eacdh individual line\n",
    "    #text = line\n",
    "    words = line.split()\n",
    "    mydict=(words,'subj')\n",
    "    bbe_tuple = tuple(mydict)\n",
    "    bbe_subj_docs.append(bbe_tuple)\n",
    "    #t=tuple(words,)\n",
    "    #print(words) #words\n",
    "   #=> ['last', 'day', 'of', 'cfiminds', 'is', 'just', 'kicking', 'off', 'today’s', 'theme', 'is', 'ai', 'and', 'intelligence', 'augmentation', 'we’re', 'starting', 'with…', 'tconizrsbb0fj']\n",
    "#print(bbe_subj_docs[0])\n",
    "n=bbe_subj_docs[0]\n",
    "a=a+1\n",
    "end_doc_row_pointer=int(len(myfinaltweets))\n",
    "b=operator.__index__(end_doc_row_pointer)\n",
    "bbe_obj_docs=[] #need a list to put the tuples in to run nB sentiment\n",
    "mydict = {} \n",
    "for line in myfinaltweets[a:b]:  #this will print eacdh individual line\n",
    "    #text = line\n",
    "    words = line.split()\n",
    "    mydict=(words,'obj')\n",
    "    bbe_tuple = tuple(mydict)\n",
    "    bbe_obj_docs.append(bbe_tuple)\n",
    "#len(bbe_subj_docs)#len(bbe_obj_docs)\n",
    "#bbe_subj_docs#bbe_obj_docs\n",
    "subj_docs =bbe_subj_docs\n",
    "obj_docs = bbe_obj_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"===================================================================\n",
    "    SENTIMMENT ANALYUSIS => VADAR\n",
    "=======================================================================\"\"\"\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#this analyzer expects a list of text sentences\n",
    "# provides positive, negative, and neutral. It also gives a compound\n",
    "#score, which should be the overall sentiment, ranging -1 to +1 (positive).\n",
    "\n",
    "sentences = myfinaltweets\n",
    "#myfinaltweets\n",
    "#len(myfinaltweets)\n",
    "sid=SentimentIntensityAnalyzer()\n",
    "score = []\n",
    "for sentence in sentences:\n",
    "    #print(sentence)\n",
    "    ss=sid.polarity_scores(sentence)\n",
    "    for k in sorted(ss):\n",
    "        #print('{0}:{1},'.format(k,ss[k]),end=' ') #single quote not a double\n",
    "        score.append(ss)\n",
    "#creteing a data frame with all the sentiment scores so I can aggregate them\n",
    "import pandas as pd\n",
    "score_result = pd.DataFrame(score)\n",
    "score_result  #uncomment it want to see the dataframe\n",
    "    #      compound    neg    neu    pos\n",
    "    #0       0.8104  0.076  0.557  0.368\n",
    "\"\"\"score the whole dataframe for a net pos/neg result\"\"\" \n",
    "#https://cmdlinetips.com/2018/12/how-to-loop-through-pandas-rows-or \\\n",
    " #                                   -how-to-iterate-over-pandas-rows/\n",
    "pos_value=0\n",
    "neg_value=0\n",
    "no_count=0\n",
    "pos_count=0\n",
    "neg_count=0\n",
    "for index, row in score_result.iterrows():\n",
    "    pos_value=0\n",
    "    neg_value=0\n",
    "    pos_value = (index,row['pos'])         \n",
    "    neg_value = (index,row['neg'])\n",
    "    if pos_value != neg_value:\n",
    "        if (pos_value > neg_value):\n",
    "            pos_count = pos_count+1\n",
    "        if (pos_value <= neg_value):\n",
    "            neg_count = neg_count+1\n",
    "    if pos_value == neg_value:  \n",
    "            no_count=no_count+1\n",
    "no_count\n",
    "pos_count\n",
    "neg_count\n",
    "print(\"..........Vadar Sentiment Score: Positive, Negative, Neutral..........\")\n",
    "print(pos_count/len(score_result)),print(neg_count/len(score_result)),print(no_count/len(score_result))\n",
    "if((pos_count/len(score_result))>neg_count/len(score_result)):\n",
    "    print(\"......................OVERALL VADAR SENTIMENT POSITIVE ! \")\n",
    "if((pos_count/len(score_result))<=neg_count/len(score_result)):\n",
    "    print(\"......................OVERALL VADAR SENTIMENT NEGATIVE ! \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
