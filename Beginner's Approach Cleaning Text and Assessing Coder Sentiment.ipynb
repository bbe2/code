{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNLTK Vadar: http://www.nltk.org/_modules/nltk/sentiment/vader.html\\n#scores text positive, negative, and neutral. It also gives a cmpd\\n#score, which should be overall sentiment, ranging from -1 (neg) to +1 (pos).\\n\\nCohen’s kappa: a statistic that measures inter-annotator agreement.\\nhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html\\n    y1array, shape = [n_samples]\\n    Labels assigned by the first annotator.\\n    y2array, shape = [n_samples]\\n    Labels assigned by the second annotator\\n\\nKrippendorff:For inter-rater agreements for experimental data with missing \\n      values, Krippendorff's alpha coef. established as a standard measure.\\n      https://github.com/grrrr/krippendorff-alpha\\n      \\n      #Text  ...        Label creation time\\n# 0  NIck's Pizza Port. Had heard about this ...  ...  May 23, 2020, 7:31 AM UTC\\n# 1  Tokyo Star Sushi. I really like this buf...  ...  May 23, 2020, 7:29 AM UTC\\n\\n#Choice a: Clearn in Excel\\n#Choice B: Clean in Python ==> simply depends if need engineering in future\\n\\n##########################################################################\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################################################\n",
    "\n",
    "#Title: Beginner's approach to Cleaning Text & Assessing Sentiment \n",
    "#By: b.hogan@snhu.edu\n",
    "#Purpose:    For training purposes: basic tweet cleaning, labeling, sentiment\n",
    "#            score w vadar, Turker Sentiment, Worker Kappa Agreement using\n",
    "#            either Cohen or Krippendorf\n",
    "\"\"\"\n",
    "NLTK Vadar: http://www.nltk.org/_modules/nltk/sentiment/vader.html\n",
    "#scores text positive, negative, and neutral. It also gives a cmpd\n",
    "#score, which should be overall sentiment, ranging from -1 (neg) to +1 (pos).\n",
    "\n",
    "Cohen’s kappa: a statistic that measures inter-annotator agreement.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html\n",
    "    y1array, shape = [n_samples]\n",
    "    Labels assigned by the first annotator.\n",
    "    y2array, shape = [n_samples]\n",
    "    Labels assigned by the second annotator\n",
    "\n",
    "Krippendorff:For inter-rater agreements for experimental data with missing \n",
    "      values, Krippendorff's alpha coef. established as a standard measure.\n",
    "      https://github.com/grrrr/krippendorff-alpha\n",
    "      \n",
    "      #Text  ...        Label creation time\n",
    "# 0  NIck's Pizza Port. Had heard about this ...  ...  May 23, 2020, 7:31 AM UTC\n",
    "# 1  Tokyo Star Sushi. I really like this buf...  ...  May 23, 2020, 7:29 AM UTC\n",
    "\n",
    "#Choice a: Clearn in Excel\n",
    "#Choice B: Clean in Python ==> simply depends if need engineering in future\n",
    "\n",
    "##########################################################################\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '0', '0', '0', '1', '1', '0', '1']\n",
      "['ick pia port had heard about this ', 'tokyo star sushi i really like this buf', 'after i went shopping with some of my fr', '\"wow olive oil garde was very disappoi', 'seventh heave restaurant was ever know', 'i went to dave restaurant and loved it', 'i went to tommy burger and pasta restaur', 'i went to the chilis o alto blvd and h', 'omg this restaurant the receptionist g']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################turk###############\n",
    "#===> Step 1: Preprocessing & Data Inspection\n",
    "####################################################################\"\"\"\n",
    "os.getcwd()\n",
    "os.chdir(\"C:\\\\Users\\\\17574\\\\Desktop\\\\Dave_Tutoriong\")\n",
    "\n",
    "#Quickly view data \n",
    "#df = pd.read_csv(\"C:\\\\Users\\\\17574\\\\Desktop\\\\Dave_Tutoriong\\\\rawdata.csv\")\n",
    "#df.columns #Index(['Text', 'Label', 'Source', 'Confidence', 'Label creation time'], dtype='object')\n",
    "#df[['Text','Label']]   #panda dataframe subtyping approach\n",
    "#df\n",
    "\"\"\" ####################################################################\n",
    "===> Step 2: Text Cleaning & Addressing Feature Removal such as stopwords\n",
    "    ####################################################################\"\"\"\n",
    "rawfilename = \"rawdata.csv\"\n",
    "FILE = open(rawfilename, \"r\")  #encoding = \"ISO-8859-1\") \n",
    "filename = \"a_mycleandata.csv\" #clean then write it back to csv!need empty csv file\n",
    "NEWFILE = open(filename, \"w\") ##in 1st row create lable & text columns\n",
    "towrite = \"Label, Text\\n\"      ##\"\"\"write this to new empty csv file\"\"\"\n",
    "NEWFILE.write(towrite)\n",
    "NEWFILE.close()\n",
    "NEWFILE = open(filename, \"a\") \n",
    "myfinaldf = pd.DataFrame()\n",
    "outputfile = \"a_details_mycleandata.txt\"\n",
    "OUTFILE = open(outputfile, \"w\") \n",
    "OUTFILE.close()\n",
    "OUTFILE = open(outputfile,\"a\")\n",
    "\n",
    "remove_stopwords=0  #set to 1 for using nltk for english stopwords\n",
    "sentences=[]\n",
    "mylabel=[]\n",
    "for row in FILE:   ##going line by line\n",
    "    rawrow=\"\\n\\nThe row is: \" + row + \"\\n\"\n",
    "    OUTFILE.write(rawrow) ##i am going to write this again later for comp\n",
    "    row = row.lstrip()  #strip all space from the left\n",
    "    row = row.rstrip()  ##strip all the space fro the right\n",
    "    row = row.strip() #strip all extra spaces in general\n",
    "    mylist = row.split(\" \")\n",
    "    #print(mylist)\n",
    "    #clean and put results in new list\n",
    "    newlist = []\n",
    "    for word in mylist:\n",
    "        #print(\"the new word is: \",word)\n",
    "        placeinoutputfile = \"The next word before is: \" + word + \"\\n\"\n",
    "        OUTFILE.write(placeinoutputfile)\n",
    "        word = word.lower()\n",
    "        word = word.lstrip()\n",
    "        word = word.strip(\"\\n\")\n",
    "        word = word.strip(\"\\\\n\")\n",
    "        word = word.replace(\",\",\"\")\n",
    "        word = word.replace(\" \",\"\")\n",
    "        word = word.replace(\"_\",\"\")\n",
    "        word = re.sub('\\+', '',word)\n",
    "        word = re.sub('.*\\+\\n','',word)   ##LOOKS FUNNY! single quotes!\n",
    "        word = re.sub('zz+','',word)\n",
    "        word = word.replace(\"\\t\",\"\")\n",
    "        word = word.replace(\".\",\"\")\n",
    "        word = word.replace(\"\\'s\",\"\")  #was comment3d out\n",
    "        word = word.strip()\n",
    "        \n",
    "        newlist.append(word)\n",
    "        #word.replace(\"\\\",\"\")  #was commented out\n",
    "        if remove_stopwords == 1: \n",
    "            if word not in[\"\",\"\\\\\",\"'\",\"*\",\":\",\";\"]:\n",
    "                if len(word) <1:\n",
    "                    if not re.search(r'\\d',word): ##remove the digits\n",
    "                        # HW2 ===non english words\n",
    "                        if remove_stopwords==1:  #code to remove nonenglish words\n",
    "                            if word in nltk_words.words():\n",
    "                                word= word\n",
    "                            else:\n",
    "                                word = \"\"\n",
    "                        if remove_stopwords==1:\n",
    "                            stop_words=set(stopwords.words(\"english\"))\n",
    "                            if word not in stop_words:\n",
    "                                word = word\n",
    "                        newlist.append(word)\n",
    "                        placeinoutputfil = \"The next word AFTER is: \" + word + \"\\n\"\n",
    "                        OUTFILE.write(placeinoutputfile)\n",
    "                        #NOW WE HAVE ALL THE WORDS   \n",
    "            ###if super stuck the following will delete the entire line and move forward\n",
    "            #if word not in[\"\",\"\\\\\",\"'\",\"*\",\":\",\";\"]:\n",
    "            #    word =\"\"\n",
    "    #print(newlist)\n",
    "    #print(newlist[-1])  #the last element = the label\n",
    "    label = newlist[-1]\n",
    "    if \"positive\" in label:   # change to \"pos\"   or \"neg\" depend on file\n",
    "        label = \"1\"\n",
    "    if \"negative\" in label:   # change to \"pos\"   or \"neg\" depend on file\n",
    "        label = \"0\"\n",
    "    mylabel.append(label)\n",
    "#    placeinoutputfile = \"\\n The label is: \" + label + \"\\n\"\n",
    "#    OUTFILE.write(placeinoutputfile)\n",
    "    \"\"\"remove the last item which is the label with pop command\"\"\"\n",
    "    newlist.pop()  \n",
    "    text = \" \".join(newlist)\n",
    "    #placeinoutputfile = \"\\n The Text is: \" + Text + \"\\n\"\n",
    "    #OUTFILE.write(placeinoutputfile)\n",
    "    #print(text)\n",
    "    #print(\"LABEL\\n\"\n",
    "    #print(label)\n",
    "    ## MORE CLEANING\n",
    "#    text = text.replace(\"\\\\n\",\"\")\n",
    "#    text = text.strip(\"\\\\n\")\n",
    "#    text = text.replace(\"\\\\'\",\"\")\n",
    "#    text = text.replace(\"\\\\\",\"\")\n",
    "#    text = text.replace(\"''\",\"\")\n",
    "#    text = text.replace(\"'\",\"\")\n",
    "#    text = text.replace(\"s'\",\"\")\n",
    "#    text = text.lstrip()\n",
    "    \n",
    "    originalrow = \"ORIGINAL\" + rawrow\n",
    "    OUTFILE.write(originalrow)\n",
    "    towrite = label+\",\"+text+\"\\n\"\n",
    "    NEWFILE.write(towrite)\n",
    "    OUTFILE.write(towrite)\n",
    "    sentences.append(text)\n",
    "    \n",
    "FILE.close()       ##alwasy  the files!\n",
    "NEWFILE.close()\n",
    "OUTFILE.close()\n",
    "print(mylabel)\n",
    "print(sentences)\n",
    "len(sentences)\n",
    "len(mylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   neg    neu    pos  compound\n",
      "0  0.0  1.000  0.000    0.0000\n",
      "1  0.0  0.682  0.318    0.4201\n",
      "2  0.0  1.000  0.000    0.0000\n",
      "3  0.0  0.612  0.388    0.5859\n",
      "4  0.0  1.000  0.000    0.0000\n",
      "5  0.0  0.606  0.394    0.5994\n",
      "6  0.0  1.000  0.000    0.0000\n",
      "7  0.0  1.000  0.000    0.0000\n",
      "8  0.0  1.000  0.000    0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\17574\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ####################################################################\n",
    "===> Step 3: Get Vadar scores to compare against Turkers, aka ground truth\n",
    "    ####################################################################\"\"\"\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#from vader_lexicon import SentimentIntensityAnalyzer\n",
    "sid=SentimentIntensityAnalyzer()\n",
    "score = []\n",
    "for sentence in sentences:\n",
    "    #print(sentence)\n",
    "    ss=sid.polarity_scores(sentence)\n",
    "    score.append(ss)\n",
    "#    for k in sorted(ss):\n",
    "#        #print('{0}:{1},'.format(k,ss[k]),end=' ') #single quote not a double\n",
    "#        score.append(ss)\n",
    "len(score)        \n",
    "vadar_score_df = pd.DataFrame(score) #use DF to aggregate\n",
    "print(vadar_score_df)\n",
    "#score_result.shape\n",
    "#df_output = pd.DataFrame(score_result)\n",
    "#df_output.to_csv(\"a_vadar_ground_truth.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Kappa Turkers and Author:\n",
      "0.28 -0.11 -0.29 0.36 0.28 -0.12 -0.29 -0.29 -0.12 -0.11 -0.26 -0.26\n",
      "Krippendorff's alpha: 1=perfect, 0=no reliability, <0 systematic disagreement -0.08455284552845543\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ######################################################################\n",
    "===> Step 4: Get Turker Kappa for agreement amongst workers\n",
    "    ######################################################################\"\"\"\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\17574\\\\Desktop\\\\Dave_Tutoriong\\\\turker_author_labels.csv\")\n",
    "df.columns\n",
    "df\n",
    "\n",
    "\n",
    "\"\"\" y stands for vector, ie a worker or author's label \"\"\"\n",
    "y1=[]; y2=[]; y3=[]; y4=[]   #vector variable\n",
    "y1 = df['turker_1'].tolist()\n",
    "y2 = df['turker_2'].tolist() #fill w data\n",
    "y3 = df['turker_3'].tolist()\n",
    "y4 = df['author'].tolist()\n",
    "y5 = abs(round(vadar_score_df['compound'],0)).tolist()  #vadar\n",
    "\"\"\"calculated the stats between pairs\"\"\"\n",
    "y1z1=round(cohen_kappa_score(y1,y2),2) #calc stats\n",
    "y1z2=round(cohen_kappa_score(y1,y3),2)\n",
    "y1z3=round(cohen_kappa_score(y1,y4),2)\n",
    "y1z4=round(cohen_kappa_score(y1,y5),2)\n",
    "y2z1=round(cohen_kappa_score(y2,y1),2) \n",
    "y2z2=round(cohen_kappa_score(y2,y3),2)\n",
    "y2z3=round(cohen_kappa_score(y2,y4),2)\n",
    "y2z4=round(cohen_kappa_score(y2,y4),2)\n",
    "y3z1=round(cohen_kappa_score(y3,y2),2) \n",
    "y3z2=round(cohen_kappa_score(y3,y1),2)\n",
    "y3z3=round(cohen_kappa_score(y3,y4),2)\n",
    "y3z4=round(cohen_kappa_score(y3,y5),2)\n",
    "print(\"Manual Kappa Turkers and Author:\")\n",
    "print(y1z1, y1z2, y1z3, y1z4,y2z1, y2z2, y2z3, y2z4,y3z1, y3z2, y3z3, y3z4) #inspect\n",
    "\n",
    "# ==> github help https://github.com/grrrr/krippendorff-alpha\n",
    "import krippendorff  #pip install krippendorff\n",
    "import numpy as np\n",
    "value_counts = np.array(df) #use a numpy array with the output for calc\n",
    "value_counts\n",
    "print(\"Krippendorff's alpha: 1=perfect, 0=no reliability, <0 systematic disagreement\",krippendorff.alpha(value_counts=value_counts, level_of_measurement='nominal'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author vs turkers kappa -0.15\n",
      "author vs turkers confusionmatrix: \n",
      " [[3 3 1]\n",
      " [0 2 0]\n",
      " [0 0 0]]\n",
      "turkers vs vadar kappa:  -0.05\n",
      "turkers vs vadar confusionmatrix: \n",
      " [[1 2 0]\n",
      " [1 2 2]\n",
      " [0 1 0]]\n",
      "Krippendorff's alpha GroundTruth:  0.07161458333333337\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ########################################################### #########\n",
    "===> Ground Truth to Vadar to Turkers\n",
    "    ####################################################################\"\"\"\n",
    "y6=round(((df['turker_1']+df['turker_2']+df['turker_3'])/3),0).tolist()\n",
    "#y4=author\n",
    "#y5=vadar\n",
    "mykappa_author_vs_turkers=cohen_kappa_score(y4,y6) #author vs turkers\n",
    "mykappa_turkers_vs_vadar = cohen_kappa_score(y6,y5)\n",
    "print(\"author vs turkers kappa\",round(mykappa_author_vs_turkers,2))\n",
    "print(\"author vs turkers confusionmatrix: \\n\",confusion_matrix(y1, y2))\n",
    "print(\"turkers vs vadar kappa: \", round(mykappa_turkers_vs_vadar,2))\n",
    "print(\"turkers vs vadar confusionmatrix: \\n\",confusion_matrix(y2, y3))\n",
    "print(\"Krippendorff's alpha GroundTruth: \", krippendorff.alpha(value_counts=value_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
