{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" #17574 b.hogan@snhu.edu\n",
    "objective: read in a text corpus from file path\n",
    "vectorize/ term matrix -> remove stops -> stemming / lemm.\n",
    "                       -> tf-ift calc  -> cluster -> k-means + other learning\n",
    "                       -> wordcloud\n",
    "\n",
    "Uses: great way to understand behind the scene details in AWS\\Google NLP compute level 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD CORPUS\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"C:\\\\Users\\17574\\Desktop\\data_it304\\shakespeare_txt\") \n",
    "\n",
    "# this method uses reading by a path\n",
    "path = \"C:\\\\Users\\\\17574\\\\Desktop\\\\data_it304\\\\shakespeare_txt  #wow need 2 slashs\n",
    "#print(os.listdir(path))\n",
    "# save the lsit\n",
    "filenamelist = os.listdir(path)\n",
    "print(type(filenamelist))  #save as a list  #check the type\n",
    "\n",
    "#need complete paths to work with CountVectorizer...CONSTRAINT OF METHOD\n",
    "listofcompletefilepaths =[]  #need an empty list\n",
    "listofjustfilenames = []\n",
    "for name in os.listdir(path):\n",
    "    #print(path+ \"\\\\\" + name)\n",
    "    next = path+ \"\\\\\" + name\n",
    "    nextnameL = name.split(\".\")\n",
    "    nextname = nextnameL[0]  #this is pretty interesting...\n",
    "    listofcompletefilepaths.append(next)\n",
    "    listofjustfilenames.append(nextname)\n",
    "#print(listofcompletefilepaths)\n",
    "#print(listofjustfilenames)\n",
    "len(listofjustfilenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTORIZE AND CREATE DOCUMENT TERM MATRIX\n",
    "\n",
    "myvect3 = CountVectorizer(input='filename')\n",
    "    #CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "    #        dtype=<class 'numpy.int64'>, encoding='utf-8', input='filename',\n",
    "    #        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
    "    #        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
    "    #        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "    #        tokenizer=None, vocabulary=None)\n",
    "x_dh = myvect3.fit_transform(listofcompletefilepaths) #vector w file names\n",
    "x_dh.shape  ## documents by the total words\n",
    "#print(x_dh) #now what do we have\n",
    "    #  (0, 6387)     1           \n",
    "    #  (0, 6056)     1\n",
    "\n",
    "#get the feature names WHICH ARE THE WORDS!  \n",
    "colnames_original = myvect3.get_feature_names()\n",
    "print(colnames_original)\n",
    "len(colnames_original)\n",
    "\n",
    "#Create  a document term model - DTM ( a matrix of counts)\n",
    "corpusDF0 = pd.DataFrame(x_dh.toarray(), columns=colnames_original)\n",
    "print(corpusDF0)\n",
    "\n",
    "#simple dictionary for filename + generic numeric ID\n",
    "mydict = {}  #now update the row names (corpus file names)\n",
    "for i in range(0, len(listofjustfilenames)):\n",
    "    mydict[i] = listofjustfilenames[i]\n",
    "print(mydict)\n",
    "\n",
    "#buildthe corpus with teh papernames based on the file names\n",
    "corpusDF0 = corpusDF0.rename(mydict, axis=\"index\")\n",
    "print(corpusDF0)\n",
    "\n",
    "df_output = pd.DataFrame(corpusDF0) ## inspection\n",
    "output_data = df_output  #output the total tweet datatable\n",
    "output_data.to_csv(\"aBBE_today.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN AND ADDRESS STOPWORDS\n",
    "\n",
    "#in pandas\n",
    "corpusDF0['zeta'] #USED FOR unknown authors...\n",
    "\n",
    "#print(\"Initial column names: \\n\", columnnames3)\n",
    "mystops = [\"also\",\"and\",\"are\",\"you\",\"of\",\"let\",\"not\",\"the\",\"for\",\"why\",\n",
    "           \"there\",\"one\",\"which\"]\n",
    "\n",
    "cleanDF = corpusDF0 # make a cleanDF to add and remove columns\n",
    "colnames_new = []   #build a new colmns list\n",
    "for name in colnames_original:\n",
    "    #print(\"FFFF\",name)\n",
    "    if((name in mystops) or (len(name)<3)):\n",
    "        #print(\"word dropping: \",name)\n",
    "        cleanDF = cleanDF.drop([name],axis=1) # drop stopword column\n",
    "        #print(cleanDF)\n",
    "    else:\n",
    "        colnames_new.append(name)\n",
    "cleanDF.shape       # Out[48]: (85, 8588) \n",
    "len(colnames_original) # origial import\n",
    "len(colnames_new) #with stopwrods removed\n",
    "colnames_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET NEW DATAFRAME COLUMNS NAMES\n",
    "\n",
    "change_tracker=[]       \n",
    "for name1 in colnames_new:  #string operations getting rid of word after letter \n",
    "    for name2 in colnames_new:  #on the right\n",
    "        if (name1 == name2):\n",
    "            print(\"skip\")\n",
    "        elif(name1.rstrip(\"e\") in name2): #thi sis good for plurals\n",
    "            change_tracker.append(name1+ \" \" + name2)\n",
    "            # like dog an dogs, but not for the hike an hiking\n",
    "            #so I will srip and \"e\" if there is one...\n",
    "            print(\"combining:\",name1, name2)\n",
    "            #print(corpusDF0[name1] + corpusDF0[name2])\n",
    "            #new = name1 + name2\n",
    "            cleanDF[name1] = cleanDF[name1] + cleanDF[name2]\n",
    "            cleanDF = cleanDF.drop([name2], axis=1) #axis 1 is columns\n",
    "change_tracker\n",
    "len(change_tracker)\n",
    "change_tracker\n",
    "print(cleanDF.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEMMING AND ADDRESSING WORD CONSOLIDATION\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stem = PorterStemmer()        #print(\"Stemmed Word:\",stem.stem(word))\n",
    "change_tracker=[]\n",
    "\n",
    "colnames[0:50] #list of words to debug this stemming...\n",
    "\n",
    "word_family = []\n",
    "skip_track=[]\n",
    "for name1 in colnames_new:  #string operations getting rid of word after letter \n",
    "    word_family\n",
    "    word1 = stem.stem(name1)\n",
    "    stem_colnames.append(name1)\n",
    "    for name2 in colnames:\n",
    "        word2 = stem.stem(name2)\n",
    "        if (word1 == word2):\n",
    "            stem_colnames.append(name2)\n",
    "            \n",
    "word_family = []                                        #########  STEMMING\n",
    "i=0\n",
    "while i <= len(colnames_new):\n",
    "    name1 = colnames[i]\n",
    "    stem1 = stem.stem(name1)\n",
    "    word_family.append(stem1)\n",
    "    i = i+1\n",
    "\n",
    "colnames_new[0:25]\n",
    "    \n",
    "stem_colnames  \n",
    "len(stem_colnames)                                    #########  STEMMING\n",
    "len(colnames_new)\n",
    "stemword_freqency = nltk.FreqDist(stem_colnames)\n",
    "for key in stemword_freqency:\n",
    "    if stemword_freqency[key] >5:\n",
    "        print(key,stemword_freqency[key])\n",
    "\n",
    "df_output = stemword_freqency.values\n",
    "df_output\n",
    "output_data = df_output  #output the total tweet datatable\n",
    "output_data.to_csv(\"aBBE_today.csv\", index=True)\n",
    "\n",
    "for name1 in colnames_new:  #string operations getting rid of word after letter \n",
    "    for name2 in colnames_new:  #on the right\n",
    "        #if (name1 == name2): #if words equal at  start word position in loop\n",
    "            #print(\"skip\")\n",
    "        if(stem.stem(name1) == stem.stem(name2)): #think should look for all subsequent \n",
    "            #change_tracker.append(name1+ \" \" + name2)\n",
    "                                        #sten cases of same word\n",
    "                                        #'abandon abandon',\n",
    "                                        #'abandon abandoned',\n",
    "                                        #'abandon abandoning',\n",
    "            change_tracker.append(name1+ \" \" + name2)\n",
    "            print(\"combining:\",name1, name2)\n",
    "            #print(corpusDF0[name1])\n",
    "            #print(corpusDF0[name2])\n",
    "            #print(corpusDF0[name1] + corpusDF0[name2])\n",
    "            cleanDF[name1] = cleanDF[name1] + cleanDF[name2]\n",
    "            cleanDF = cleanDF.drop([name2], axis=1) #axis 1 is columns\n",
    "change_tracker           \n",
    "cleanDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL CLEANED NEW DATA FRAME\n",
    "cleanDF.iat[1,1]  #THIS WORKS  cleanDF['zeta'] #this works here\n",
    "\n",
    "doc=[]   \n",
    "authorYN=[]                            \n",
    "for x in range(0, len(cleanDF)):\n",
    "    y = cleanDF.columns.get_loc(\"author_1\") #get column index\n",
    "    y1 = cleanDF.columns.get_loc(\"author_2\")\n",
    "        if cleanDF.iat[x,y] == 1:  #disputed  data brought in\n",
    "        z = cleanDF.iat[x,y]\n",
    "        authorYN = 0\n",
    "    if cleanDF.iat[x,y1] == 1: #author_1\n",
    "        z = 2\n",
    "        authorYN= 1\n",
    "    if cleanDF.iat[x,y2] == 1: #author_2\n",
    "        z = 3\n",
    "        authorYN= 1\n",
    "#    y2 = cleanDF.columns.get_loc(\"author_3\")\n",
    "#    y3 = cleanDF.columns.get_loc(\"author_4\")\n",
    "#   if cleanDF.iat[x,y3] == 1: #author_3\n",
    "#        z = 4\n",
    "#        authorYN= 1                 #author_4\n",
    "#    if cleanDF.iat[x,y1] == 1 and cleanDF.iat[x,y3] == 1:\n",
    "#        z = 5\n",
    "#        authorYN= 1       \n",
    "    doc.append(z)\n",
    "    authorYN.append(authorYN)\n",
    "    z=0\n",
    "    authorYN=99\n",
    "    \n",
    "documents = pd.DataFrame(doc)\n",
    "documents = documents.rename(mydict, axis=\"index\")\n",
    "documents = documents.rename(columns={0:'doc'})\n",
    "documents.head()\n",
    "\n",
    "#dataframe for authorYN label\n",
    "authoryn = pd.DataFrame(authorYN)\n",
    "authoryn = authoryn.rename(mydict, axis=\"index\")\n",
    "authoryn = authoryn.rename(columns={0:'authorYN'})\n",
    "authoryn.head(12)\n",
    "\n",
    "#update the source dataframe with the new settings for\n",
    "cleanDF['zeta'] = authoryn['authorYN']\n",
    "z=cleanDF.columns.get_loc(\"zeta\") #get column index\n",
    "cleanDF = cleanDF.rename(columns={'zeta':'authorYN'})\n",
    "cleanDF.head()\n",
    "\n",
    "testDF= cleanDF\n",
    "##add labels back into the dataframe\n",
    "testDF = documents.to_frame() #index to 0  #thi sis interesting!\n",
    "print(type(documents))\n",
    "testDF.index = documents.index - 1\n",
    "#print(new_labels)\n",
    "labeledclean_DF[\"Label\"] = new_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-1-e3574da3771e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-e3574da3771e>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    \\#Normalization\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "#LONG HAND CODING OF TF-IDF\n",
    "# often use sci-kit learn as well\n",
    "\n",
    "import math\n",
    "\n",
    "df_data = pd.DataFrame(cleanDF).values.astype(int)\n",
    "df_data\n",
    "#transpose the frame\n",
    "df_data_transposed = df_data.T  #transpose the frame\n",
    "df_data_transposed[0]\n",
    "df_data_transposed.shape[1] ## of words transposed, want 1 for docs\n",
    "\n",
    "mydocfreq=[]   #word counts across the documents\n",
    "for x in range(0,len(df_data_transposed)):\n",
    "    wf = int(sum(df_data_transposed[x])) #[x]\n",
    "    idf = wf / df_data_transposed.shape[1]  #number of docs\n",
    "    mydocfreq.append(idf)\n",
    "    wf=\"\"\n",
    "    idf = \"\"\n",
    "\n",
    "df_mydocfreq_inverse = pd.DataFrame(mydocfreq).values.astype(float)\n",
    "df_mydocfreq = pd.DataFrame(df_mydocfreq_inverse).T  #thats right make 1 x 1384\n",
    "df_mydocfreq\n",
    "\n",
    "df_tfidf = pd.DataFrame(df_data).values.astype(float) #build frame\n",
    "df_tfidf.shape\n",
    "#zero out the dataframe - I DOULBLe checked this owrking\n",
    "for x in range(0,len(df_tfidf)): # \n",
    "    y=0\n",
    "    #demoninator = float(df_mytotalword_perdoc[x])\n",
    "    while y <= (df_tfidf.shape[1]-1):  #shape gives the y dimension of columns\n",
    "        df_tfidf[x,y] = 0\n",
    "        y +=1\n",
    "#####===> TF-IDF the data\n",
    "for x in range(0,len(df_data)): # rows in data frame\n",
    "    y=0\n",
    "    while y <= (df_tfidf.shape[1]-1):  #shape gives the y dimension of columns\n",
    "        df_tfidf[x,y] = df_data[x,y]* math.log(mydocfreq[y]) \n",
    "        y +=1 \n",
    "df_tfidf.shape\n",
    "\n",
    "#export back to Excel\n",
    "DF_Homework = pd.DataFrame(df_tfidf)\n",
    "output_data = DF_Homework  #output the total table\n",
    "output_data.to_csv(\"aBBE_inspect.csv\", index=True)\n",
    "\n",
    "labeledclean_DF =pd.DataFrame(df_tfidf,columns=colnames_new)\n",
    "labeledclean_DF = labeledclean_DF.rename(mydict, axis=\"index\")\n",
    "labeledclean_DF['zeta'] = authoryn['authorYN']\n",
    "labeledclean_DF['128'] = documents['doc']\n",
    "labeledclean_DF = labeledclean_DF.rename(columns={'zeta':'authorYN'})\n",
    "labeledclean_DF = labeledclean_DF.rename(columns={'128':'Doc'})\n",
    "labeledclean_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTERING\n",
    "\n",
    "print(type(labeledclean_DF)) #check the type is a dataframe\n",
    "from sklearn.cluster import KMeans#Using SKlearn - - WOWSERS IS THSI FAST...\n",
    "import numpy as np #kmeans_object = sklearn.cluster.KMeans(n_clusters=3)\n",
    "#print(kmeans_object)\n",
    "    #KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
    "    #    n_clusters=3, n_init=10, n_jobs=None, precompute_distances='auto',\n",
    "    #    random_state=None, tol=0.0001, verbose=0)\n",
    "#K-means model\n",
    "mymatrix_data = labeledclean_DF.values #matrix of k-means data\n",
    "kmeans_object = KMeans(n_clusters=4)   #tyring 3 and 4 clusters\n",
    "kmeans_object.fit(mymatrix_data)       #fit model\n",
    "labels = kmeans_object.labels_   #get cluster assignment labels\n",
    "#Build Results\n",
    "myresults = pd.DataFrame([corpusDF0.index,labels]).T #format results as DF\n",
    "myresults = myresults.rename(mydict, axis=\"index\")    #add column to merge\n",
    "myresults = myresults.rename(columns={1:'k-means-label'}) #renaming\n",
    "myresults = myresults.rename(columns={0:'docname'})\n",
    "myresults.head()\n",
    "documents = pd.DataFrame(doc) #original list of the documents from import\n",
    "documents = pd.DataFrame([corpusDF0.index,labels]).T #add column to merge\n",
    "documents = documents.rename(columns={1:'authorID'})  #renaming\n",
    "documents = documents.rename(columns={0:'docname'})\n",
    "documents.head()\n",
    "#Merge the results\n",
    "finalDF = myresults.merge(documents, on='docname')  #yippeeeeeeeee  !!!!\n",
    "finalDF\n",
    "from pandas_ml import ConfusionMatrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_actual=[]\n",
    "y_predict=[]\n",
    "y = finalDF.columns.get_loc(\"authorID\") #get column index\n",
    "y1 = finalDF.columns.get_loc(\"k-means-label\")\n",
    "y\n",
    "for x in range(0,len(finalDF)):\n",
    "    y_actual.append(finalDF.iat[x,y])\n",
    "    y_predict.append(finalDF.iat[x,y])\n",
    "y_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEATMAP AND CONFUSION MATRIX OF K-MEANS RESULTS\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_actual,y_predict)\n",
    "confusion_matrix\n",
    "\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "df_cm = pd.DataFrame(confusion_matrix, range(4),range(4))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm,annot=True,annot_kws={\"size\":16}) #font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORDCLOUD ACROSS CORPUS\n",
    "\n",
    "listofjustfilenames[0] #GET LIST of file data names\n",
    "mycorpus_data=[]\n",
    "for i in range(0,len(listofjustfilenames)):\n",
    "    filename = open(listofjustfilenames[i] + \".txt\",\"r\")\n",
    "    for line in filename:\n",
    "        textline = line.strip()\n",
    "        mycorpus_data.append(textline)  \n",
    "    filename.close()\n",
    "len(mycorpus_data)\n",
    "#inspecting file names in excel to make s graph\n",
    "df_output = pd.DataFrame(mycorpus_data)\n",
    "output_data = df_output  #output the total tweet datatable\n",
    "output_data.to_csv(\"corpus.csv\", index=True)\n",
    "\n",
    "mycorpus_data\n",
    "wordlist = []  # join all\n",
    "wordlist = \" \".join(mycorpus_data) \n",
    "wordlist\n",
    "tokenized_word=word_tokenize(wordlist)\n",
    "len(tokenized_word)\n",
    "tokenized_word\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "corpus_no_stopwords=[]\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        corpus_no_stopwords.append(w)\n",
    "#==> 3) word Frequency\n",
    "len(corpus_no_stopwords)\n",
    "corpus_no_stopwords\n",
    "\n",
    "import re  #now perform more cleaning\n",
    "mystops = [\"also\",\"and\",\"are\",\"you\",\"of\",\"let\",\"not\",\"the\",\"for\",\"why\",\"there\",\"one\",\"which\"]\n",
    "newlist = []\n",
    "for word in corpus_no_stopwords:\n",
    "    #print(\"the new word is: \",word)\n",
    "    #placeinoutputfile = \"The next word before is: \" + word + \"\\n\"\n",
    "    #OUTFILE.write(placeinoutputfile)\n",
    "    word = word.lower()\n",
    "    word = word.lstrip()\n",
    "    word = word.strip(\"\\n\")\n",
    "    word = word.strip(\"\\\\n\")\n",
    "    word = word.replace(\",\",\"\")\n",
    "    word = word.replace(\" \",\"\")\n",
    "    word = word.replace(\"_\",\"\")\n",
    "    word = re.sub('\\+', '',word)\n",
    "    word = re.sub('.*\\+\\n','',word)   ##LOOKS FUNNY! single quotes!\n",
    "    word = re.sub('zz+','',word)\n",
    "    word = word.replace(\"\\t\",\"\")\n",
    "    word = word.replace(\".\",\"\")\n",
    "    word = word.replace(\"\\'s\",\"\")  #was comment3d out\n",
    "    word = word.strip()\n",
    "    ##word.replace(\"\\\",\"\")  #was commented out\n",
    "    #if((name in mystops) or (len(name)<3)):\n",
    "    if ((word not in[\"\",\"\\\\\",\"'\",\"*\",\":\",\";\"]) or (word not in mystops)):\n",
    "        if len(word) >=3:\n",
    "            if not re.search(r'\\d',word): ##remove the digits\n",
    "                # HW2 ===non english words\n",
    "                newlist.append(word)\n",
    "                #placeinoutputfil = \"The next word AFTER is: \" + word + \"\\n\"\n",
    "                #OUTFILE.write(placeinoutputfile)\n",
    "len(corpus_no_stopwords)\n",
    "len(newlist)\n",
    "newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORDCLOUD MOST FREQUENT WORDS\n",
    "\n",
    "mostfrequentwords = nltk.FreqDist(newlist)\n",
    "mostfrequentwords\n",
    "top_words=mostfrequentwords.most_common(200) #words used most in the tweets\n",
    "DF_topwords = pd.DataFrame(top_words)\n",
    "print(\"....50 Top Words from Tweets. \\n\",DF_topwords)\n",
    "top_words\n",
    "wordcloud_items=[] #make a dictionary  ====>move to dictionary in future\n",
    "for word, freq in top_words:   #print the most commone words\n",
    "        print(\"Word:\",word,freq)\n",
    "        wordcloud_items.append(word)\n",
    "print(wordcloud_items)\n",
    "from PIL import Image\n",
    "#>conda install -c conda-forge wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "wordcloud_items = \" \".join(wordcloud_items)  ##  join\n",
    "#print(joinedfilteredtweets)  # lower max_font_size, change the maximum number of word and \n",
    "    #lighten the background:\"\"\"                                 #white, purple, etc \n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"purple\").generate(wordcloud_items)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
